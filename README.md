# RefineBoost: Enhancing the Security and Utility of Large Language Models with Minimal Data
Large language models have made remarkable progress in problem-solving and analytical tasks, thanks to extensive pre-training on vast datasets. However, these datasets may include harmful or biased content, resulting in outputs that contradict human ethical standards. Ensuring the safety and ethical alignment of these models is therefore essential for their broader use. Although reinforcement learning is commonly employed to enhance safety alignment, it often requires large datasets for fine-tuning, which may contain redundant or low-quality samples, thus diminishing the overall effectiveness of the alignment process.

To overcome this limitation, we introduce RefineBoost, a novel method designed to improve safety alignment with a smaller, high-quality dataset. RefineBoost emphasizes data quality and diversity, using a hierarchical mechanism to automatically filter and enhance datasets. Experimental results show that models fine-tuned with this method demonstrate superior safety and utility compared to those trained on full, unfiltered datasets.

We have applied this method to refine and enhance existing safety alignment datasets, and we have made these improved datasets publicly available. The open-source datasets contain both high-quality data and enhanced samples processed through our hierarchical filtering mechanism. We hope that by providing these resources, other researchers will find valuable tools to further the development of model safety alignment.
