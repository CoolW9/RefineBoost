# RefineBoost
Large language models have achieved significant advancements in problem-solving and analysis, aided by extensive pre-training datasets. However, these datasets may contain harmful content, leading to outputs that conflict with human ethics. Ensuring the safety of these models is crucial for their widespread adoption. While reinforcement learning is commonly used for safety alignment, it often requires large amounts of data to fine-tune, which may include redundancy and low-quality samples, thereby affecting the alignment effect. To address this, we propose RefineBoost, a method that promotes safety alignment with a smaller, high-quality dataset. RefineBoost focuses on data quality and diversity, employing a hierarchical mechanism to automatically filter and enhance high-quality, diverse data. Experimental results demonstrate that fine-tuned models using these data have superior safety and utility compared to models fine-tuned on full datasets.
